#pragma once

// x86-64 assembler/emitter
// auto generated. Do not edit this file manually

typedef unsigned long long u64;
typedef unsigned int u32;
typedef unsigned short u16;
typedef unsigned char u8;
typedef signed long long s64;
typedef signed int s32;
typedef signed short s16;
typedef signed char s8;

enum X86Reg : sint8
{
	X86_REG_NONE = -1,
	X86_REG_EAX = 0,
	X86_REG_ECX = 1,
	X86_REG_EDX = 2,
	X86_REG_EBX = 3,
	X86_REG_ESP = 4,
	X86_REG_EBP = 5,
	X86_REG_ESI = 6,
	X86_REG_EDI = 7,
	X86_REG_R8D = 8,
	X86_REG_R9D = 9,
	X86_REG_R10D = 10,
	X86_REG_R11D = 11,
	X86_REG_R12D = 12,
	X86_REG_R13D = 13,
	X86_REG_R14D = 14,
	X86_REG_R15D = 15,
	X86_REG_RAX = 0,
	X86_REG_RCX = 1,
	X86_REG_RDX = 2,
	X86_REG_RBX = 3,
	X86_REG_RSP = 4,
	X86_REG_RBP = 5,
	X86_REG_RSI = 6,
	X86_REG_RDI = 7,
	X86_REG_R8 = 8,
	X86_REG_R9 = 9,
	X86_REG_R10 = 10,
	X86_REG_R11 = 11,
	X86_REG_R12 = 12,
	X86_REG_R13 = 13,
	X86_REG_R14 = 14,
	X86_REG_R15 = 15
};

enum X86Cond : u8
{
	X86_CONDITION_O = 0,
	X86_CONDITION_NO = 1,
	X86_CONDITION_B = 2,
	X86_CONDITION_NB = 3,
	X86_CONDITION_Z = 4,
	X86_CONDITION_NZ = 5,
	X86_CONDITION_BE = 6,
	X86_CONDITION_NBE = 7,
	X86_CONDITION_S = 8,
	X86_CONDITION_NS = 9,
	X86_CONDITION_PE = 10,
	X86_CONDITION_PO = 11,
	X86_CONDITION_L = 12,
	X86_CONDITION_NL = 13,
	X86_CONDITION_LE = 14,
	X86_CONDITION_NLE = 15
};
class x86Assembler64
{
private:
	std::vector<u8> m_buffer;

public:
	u8* GetBufferPtr() { return m_buffer.data(); };
	std::span<u8> GetBuffer() { return m_buffer; };
	u32 GetWriteIndex() { return (u32)m_buffer.size(); };
	void _emitU8(u8 v) { m_buffer.emplace_back(v); };
	void _emitU16(u16 v) { size_t writeIdx = m_buffer.size(); m_buffer.resize(writeIdx + 2); *(u16*)(m_buffer.data() + writeIdx) = v; };
	void _emitU32(u32 v) { size_t writeIdx = m_buffer.size(); m_buffer.resize(writeIdx + 4); *(u32*)(m_buffer.data() + writeIdx) = v; };
	void _emitU64(u64 v) { size_t writeIdx = m_buffer.size(); m_buffer.resize(writeIdx + 8); *(u64*)(m_buffer.data() + writeIdx) = v; };
	using GPR64 = X86Reg;
	using GPR32 = X86Reg;
	using GPR8_REX = X86Reg;
	void ADD_bb(GPR8_REX dst, GPR8_REX src)
	{
		if ((src >= 4) || (dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x00);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void ADD_bb_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR8_REX src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src >= 4) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x00);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void ADD_bb_r(GPR8_REX dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst >= 4) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x02);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void ADD_dd(GPR32 dst, GPR32 src)
	{
		if (((src & 8) != 0) || ((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x01);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void ADD_qq(GPR64 dst, GPR64 src)
	{
		_emitU8(0x48 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		_emitU8(0x01);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void ADD_dd_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR32 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src & 8) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x01);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void ADD_qq_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR64 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x01);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void ADD_dd_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x03);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void ADD_qq_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x03);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_bb(GPR8_REX dst, GPR8_REX src)
	{
		if ((src >= 4) || (dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x30);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void XOR_bb_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR8_REX src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src >= 4) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x30);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_bb_r(GPR8_REX dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst >= 4) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x32);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_dd(GPR32 dst, GPR32 src)
	{
		if (((src & 8) != 0) || ((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x31);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void XOR_qq(GPR64 dst, GPR64 src)
	{
		_emitU8(0x48 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		_emitU8(0x31);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void XOR_dd_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR32 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src & 8) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x31);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_qq_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR64 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x31);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_dd_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x33);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void XOR_qq_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x33);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_bb(GPR8_REX dst, GPR8_REX src)
	{
		if ((src >= 4) || (dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x38);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void CMP_bb_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR8_REX src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src >= 4) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x38);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_bb_r(GPR8_REX dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst >= 4) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x3a);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_dd(GPR32 dst, GPR32 src)
	{
		if (((src & 8) != 0) || ((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x39);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void CMP_qq(GPR64 dst, GPR64 src)
	{
		_emitU8(0x48 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		_emitU8(0x39);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void CMP_dd_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR32 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src & 8) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x39);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_qq_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR64 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x39);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_dd_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x3b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_qq_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x3b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void CMP_di32(GPR32 dst, s32 imm)
	{
		if (((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3));
		}
		_emitU8(0x81);
		_emitU8((3 << 6) | ((7 & 7) << 3) | (dst & 7));
		_emitU32((u32)imm);
	}
	void CMP_qi32(GPR64 dst, s32 imm)
	{
		_emitU8(0x48 | ((dst & 8) >> 3));
		_emitU8(0x81);
		_emitU8((3 << 6) | ((7 & 7) << 3) | (dst & 7));
		_emitU32((u32)imm);
	}
	void CMP_di32_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s32 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((memReg & 8))
				_emitU8(0x40 | ((memReg & 8) >> 1));
		}
		_emitU8(0x81);
		_emitU8((mod << 6) | ((7 & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU32((u32)imm);
	}
	void CMP_qi32_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s32 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x81);
		_emitU8((mod << 6) | ((7 & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU32((u32)imm);
	}
	void CMP_di8(GPR32 dst, s8 imm)
	{
		if (((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3));
		}
		_emitU8(0x83);
		_emitU8((3 << 6) | ((7 & 7) << 3) | (dst & 7));
		_emitU8((u8)imm);
	}
	void CMP_qi8(GPR64 dst, s8 imm)
	{
		_emitU8(0x48 | ((dst & 8) >> 3));
		_emitU8(0x83);
		_emitU8((3 << 6) | ((7 & 7) << 3) | (dst & 7));
		_emitU8((u8)imm);
	}
	void CMP_di8_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s8 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((memReg & 8))
				_emitU8(0x40 | ((memReg & 8) >> 1));
		}
		_emitU8(0x83);
		_emitU8((mod << 6) | ((7 & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU8((u8)imm);
	}
	void CMP_qi8_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s8 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x83);
		_emitU8((mod << 6) | ((7 & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU8((u8)imm);
	}
	void TEST_bb(GPR8_REX dst, GPR8_REX src)
	{
		if ((src >= 4) || (dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x84);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void TEST_bb_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR8_REX src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src >= 4) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x84);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void TEST_dd(GPR32 dst, GPR32 src)
	{
		if (((src & 8) != 0) || ((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x85);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void TEST_qq(GPR64 dst, GPR64 src)
	{
		_emitU8(0x48 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		_emitU8(0x85);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void TEST_dd_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR32 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src & 8) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x85);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void TEST_qq_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR64 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x85);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_bb(GPR8_REX dst, GPR8_REX src)
	{
		if ((src >= 4) || (dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x88);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void MOV_bb_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR8_REX src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src >= 4) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x88);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_bb_r(GPR8_REX dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst >= 4) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst >= 4) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x8a);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_dd(GPR32 dst, GPR32 src)
	{
		if (((src & 8) != 0) || ((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		}
		_emitU8(0x89);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void MOV_qq(GPR64 dst, GPR64 src)
	{
		_emitU8(0x48 | ((dst & 8) >> 3) | ((src & 8) >> 1));
		_emitU8(0x89);
		_emitU8((3 << 6) | ((src & 7) << 3) | (dst & 7));
	}
	void MOV_dd_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR32 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((src & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((src & 8) || (memReg & 8))
				_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x89);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_qq_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, GPR64 src)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((src & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x89);
		_emitU8((mod << 6) | ((src & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_dd_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x8b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_qq_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x8b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void MOV_di32(GPR32 dst, s32 imm)
	{
		if (((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3));
		}
		_emitU8(0xb8 | ((dst) & 7));
		_emitU32((u32)imm);
	}
	void MOV_qi64(GPR64 dst, s64 imm)
	{
		_emitU8(0x48 | ((dst & 8) >> 3));
		_emitU8(0xb8 | ((dst) & 7));
		_emitU64((u64)imm);
	}
	void CALL_q(GPR64 dst)
	{
		if (((dst & 8) != 0))
		{
			_emitU8(0x40 | ((dst & 8) >> 3));
		}
		_emitU8(0xff);
		_emitU8((3 << 6) | ((2 & 7) << 3) | (dst & 7));
	}
	void CALL_q_l(GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((memReg & 8))
				_emitU8(0x40 | ((memReg & 8) >> 1));
		}
		_emitU8(0xff);
		_emitU8((mod << 6) | ((2 & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
	void IMUL_ddi32(GPR32 dst, GPR32 src, s32 imm)
	{
		if (((dst & 8) != 0) || ((src & 8) != 0))
		{
			_emitU8(0x40 | ((src & 8) >> 3) | ((dst & 8) >> 1));
		}
		_emitU8(0x69);
		_emitU8((3 << 6) | ((dst & 7) << 3) | (src & 7));
		_emitU32((u32)imm);
	}
	void IMUL_qqi32(GPR64 dst, GPR64 src, s32 imm)
	{
		_emitU8(0x48 | ((src & 8) >> 3) | ((dst & 8) >> 1));
		_emitU8(0x69);
		_emitU8((3 << 6) | ((dst & 7) << 3) | (src & 7));
		_emitU32((u32)imm);
	}
	void IMUL_ddi32_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s32 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x69);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU32((u32)imm);
	}
	void IMUL_qqi32_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s32 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x69);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU32((u32)imm);
	}
	void IMUL_ddi8(GPR32 dst, GPR32 src, s8 imm)
	{
		if (((dst & 8) != 0) || ((src & 8) != 0))
		{
			_emitU8(0x40 | ((src & 8) >> 3) | ((dst & 8) >> 1));
		}
		_emitU8(0x6b);
		_emitU8((3 << 6) | ((dst & 7) << 3) | (src & 7));
		_emitU8((u8)imm);
	}
	void IMUL_qqi8(GPR64 dst, GPR64 src, s8 imm)
	{
		_emitU8(0x48 | ((src & 8) >> 3) | ((dst & 8) >> 1));
		_emitU8(0x6b);
		_emitU8((3 << 6) | ((dst & 7) << 3) | (src & 7));
		_emitU8((u8)imm);
	}
	void IMUL_ddi8_r(GPR32 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s8 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((dst & 8) || (memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((dst & 8) || (memReg & 8))
				_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1));
		}
		_emitU8(0x6b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU8((u8)imm);
	}
	void IMUL_qqi8_r(GPR64 dst, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler, s8 imm)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 3) | ((index & 8) >> 2) | 0x08);
		}
		else
		{
			_emitU8(0x40 | ((dst & 8) >> 1) | ((memReg & 8) >> 1) | 0x08);
		}
		_emitU8(0x6b);
		_emitU8((mod << 6) | ((dst & 7) << 3) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
		_emitU8((u8)imm);
	}
	void Jcc_j32(X86Cond cond, s32 imm)
	{
		_emitU8(0x0f);
		_emitU8(0x80 | (u8)cond);
		_emitU32((u32)imm);
	}
	void SETcc_b(X86Cond cond, GPR8_REX dst)
	{
		if ((dst >= 4))
		{
			_emitU8(0x40 | ((dst & 8) >> 3));
		}
		_emitU8(0x0f);
		_emitU8(0x90 | (u8)cond);
		_emitU8((3 << 6) | (dst & 7));
	}
	void SETcc_b_l(X86Cond cond, GPR64 memReg, sint32 offset, GPR64 index, uint8 scaler)
	{
		uint8 mod;
		if (offset == 0 && (memReg & 7) != 5) mod = 0;
		else if (offset == (s32)(s8)offset) mod = 1;
		else mod = 2;
		bool sib_use = (scaler != 0 && index != X86_REG_NONE) || ((memReg & 7) == 4);
		if (sib_use)
		{
			if ((memReg & 8) || ((index != X86_REG_NONE) && (index & 8)))
				_emitU8(0x40 | ((memReg & 8) >> 3) | ((index & 8) >> 2));
		}
		else
		{
			if ((memReg & 8))
				_emitU8(0x40 | ((memReg & 8) >> 1));
		}
		_emitU8(0x0f);
		_emitU8(0x90);
		_emitU8((mod << 6) | (sib_use ? 4 : (memReg & 7)));
		if (sib_use)
		{
			_emitU8((0 << 6) | ((memReg & 7)) | ((index & 7) << 3));
		}
		if (mod == 1) _emitU8((u8)offset);
		else if (mod == 2) _emitU32((u32)offset);
	}
};
